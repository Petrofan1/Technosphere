{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Основная модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from collections import defaultdict\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.metrics import balanced_accuracy_score, confusion_matrix\n",
    "from sklearn.preprocessing  import LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from scipy.spatial.distance import cosine\n",
    "from numpy.linalg import norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save(obj, filename):\n",
    "    with open(filename + \".pkl\", \"wb\") as out:\n",
    "        pickle.dump(obj, out, 2)\n",
    "def load(filename):\n",
    "    with open(filename + \".pkl\", \"rb\") as inp:\n",
    "        return pickle.load(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pensez à la communication , le discours , les ...</td>\n",
       "      <td>fr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Můžete si ji pronajmout , vzít na splátky , ko...</td>\n",
       "      <td>cs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Každý starosta pochopil , že když mají tyto fo...</td>\n",
       "      <td>cs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Det är ytterligare bevis , men ändå — Jag krit...</td>\n",
       "      <td>sv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>كان الأمر لا يصدق .</td>\n",
       "      <td>ar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3159628</th>\n",
       "      <td>そんな所で捕まっている片手は 70億の人々と繫がる命綱なのです</td>\n",
       "      <td>ja</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3159629</th>\n",
       "      <td>Първоначално се опитах да направя думите &amp;quot...</td>\n",
       "      <td>bg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3159630</th>\n",
       "      <td>Ho appreso che ha a che fare con l&amp;apos; atten...</td>\n",
       "      <td>it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3159631</th>\n",
       "      <td>E os edifícios não se limitam a apenas evocar ...</td>\n",
       "      <td>pt-br</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3159632</th>\n",
       "      <td>La « Danse d ’ entrée du mariage JK » est deve...</td>\n",
       "      <td>fr-ca</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3159633 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  sentence language\n",
       "0        Pensez à la communication , le discours , les ...       fr\n",
       "1        Můžete si ji pronajmout , vzít na splátky , ko...       cs\n",
       "2        Každý starosta pochopil , že když mají tyto fo...       cs\n",
       "3        Det är ytterligare bevis , men ändå — Jag krit...       sv\n",
       "4                                      كان الأمر لا يصدق .       ar\n",
       "...                                                    ...      ...\n",
       "3159628                    そんな所で捕まっている片手は 70億の人々と繫がる命綱なのです       ja\n",
       "3159629  Първоначално се опитах да направя думите &quot...       bg\n",
       "3159630  Ho appreso che ha a che fare con l&apos; atten...       it\n",
       "3159631  E os edifícios não se limitam a apenas evocar ...    pt-br\n",
       "3159632  La « Danse d ’ entrée du mariage JK » est deve...    fr-ca\n",
       "\n",
       "[3159633 rows x 2 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv(\"data/train.csv\")\n",
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Будем искать ошибки в разметке. Посмотрим, встречаются ли какие-то тексты несколько раз. Для этого отсортируем тексты по частоте"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>що це тому ми чи дуже які щоб дякую якщо було ...</td>\n",
       "      <td>75000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>бұл мен біз бір үшін ол деп және емес бар кере...</td>\n",
       "      <td>50000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>гэта што калі мы не яны як але ён каб дзякуй б...</td>\n",
       "      <td>50000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>the and of it that you to apos we this is in t...</td>\n",
       "      <td>50000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>это что мы не как на вы они но из то он так дл...</td>\n",
       "      <td>50000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>що це ми не як на вони та але ви до про він оп...</td>\n",
       "      <td>50000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>笑声 掌声 谢谢 现在 所以 事实上 当然 鼓掌 但是 因此 那么 非常感谢 谢谢大家 是的 好吧</td>\n",
       "      <td>25000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>笑聲 掌聲 謝謝 所以 現在 事實上 當然 因此 謝謝大家 對吧 但是 鼓掌 謝謝各位 他說 我說</td>\n",
       "      <td>25000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>( Aplausos )</td>\n",
       "      <td>1279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>( Applaus )</td>\n",
       "      <td>1067</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               index  sentence\n",
       "0  що це тому ми чи дуже які щоб дякую якщо було ...     75000\n",
       "1  бұл мен біз бір үшін ол деп және емес бар кере...     50000\n",
       "2  гэта што калі мы не яны як але ён каб дзякуй б...     50000\n",
       "3  the and of it that you to apos we this is in t...     50000\n",
       "4  это что мы не как на вы они но из то он так дл...     50000\n",
       "5  що це ми не як на вони та але ви до про він оп...     50000\n",
       "6  笑声 掌声 谢谢 现在 所以 事实上 当然 鼓掌 但是 因此 那么 非常感谢 谢谢大家 是的 好吧     25000\n",
       "7  笑聲 掌聲 謝謝 所以 現在 事實上 當然 因此 謝謝大家 對吧 但是 鼓掌 謝謝各位 他說 我說     25000\n",
       "8                                       ( Aplausos )      1279\n",
       "9                                        ( Applaus )      1067"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_count = (\n",
    "    train_df.sentence.value_counts()\n",
    "    .to_frame()\n",
    "    .reset_index()\n",
    ")\n",
    "sent_count.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Действительно, кто-то желает подпортить нам обучение. Избавимся от текстов котрые встречаются много раз, а именно от первых восьми в sent_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Pensez à la communication , le discours , les ...</td>\n",
       "      <td>fr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Můžete si ji pronajmout , vzít na splátky , ko...</td>\n",
       "      <td>cs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Každý starosta pochopil , že když mají tyto fo...</td>\n",
       "      <td>cs</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Det är ytterligare bevis , men ändå — Jag krit...</td>\n",
       "      <td>sv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>كان الأمر لا يصدق .</td>\n",
       "      <td>ar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2784628</th>\n",
       "      <td>そんな所で捕まっている片手は 70億の人々と繫がる命綱なのです</td>\n",
       "      <td>ja</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2784629</th>\n",
       "      <td>Първоначално се опитах да направя думите &amp;quot...</td>\n",
       "      <td>bg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2784630</th>\n",
       "      <td>Ho appreso che ha a che fare con l&amp;apos; atten...</td>\n",
       "      <td>it</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2784631</th>\n",
       "      <td>E os edifícios não se limitam a apenas evocar ...</td>\n",
       "      <td>pt-br</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2784632</th>\n",
       "      <td>La « Danse d ’ entrée du mariage JK » est deve...</td>\n",
       "      <td>fr-ca</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2784633 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  sentence language\n",
       "0        Pensez à la communication , le discours , les ...       fr\n",
       "1        Můžete si ji pronajmout , vzít na splátky , ko...       cs\n",
       "2        Každý starosta pochopil , že když mají tyto fo...       cs\n",
       "3        Det är ytterligare bevis , men ändå — Jag krit...       sv\n",
       "4                                      كان الأمر لا يصدق .       ar\n",
       "...                                                    ...      ...\n",
       "2784628                    そんな所で捕まっている片手は 70億の人々と繫がる命綱なのです       ja\n",
       "2784629  Първоначално се опитах да направя думите &quot...       bg\n",
       "2784630  Ho appreso che ha a che fare con l&apos; atten...       it\n",
       "2784631  E os edifícios não se limitam a apenas evocar ...    pt-br\n",
       "2784632  La « Danse d ’ entrée du mariage JK » est deve...    fr-ca\n",
       "\n",
       "[2784633 rows x 2 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mistakes = list(sent_count.iloc[:8][\"index\"])\n",
    "for item in mistakes:\n",
    "    train_df = train_df[(train_df[\"sentence\"] != item)]\n",
    "train_df.reset_index(drop=True, inplace=True)\n",
    "train_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отделяем целевую переменную от остального датасета и кодируем ее значения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = train_df.language\n",
    "train_df.drop(columns=['language'], inplace=True)\n",
    "label_encoder = LabelEncoder().fit(target)\n",
    "y = label_encoder.transform(target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Составляем pipeline и обучаем нашу модель. В качество тоекнов будут выступать n-грамы, где n варьируется от 2 до 4. Не забываем добавить балансировку, так как классы не сбалансированные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline([\n",
    "    ('vectorizer', TfidfVectorizer(analyzer=\"char_wb\", ngram_range=(2, 4))),\n",
    "    ('model', SGDClassifier(random_state=42, loss='modified_huber', class_weight='balanced'))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 35min 54s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('vectorizer',\n",
       "                 TfidfVectorizer(analyzer='char_wb', binary=False,\n",
       "                                 decode_error='strict',\n",
       "                                 dtype=<class 'numpy.float64'>,\n",
       "                                 encoding='utf-8', input='content',\n",
       "                                 lowercase=True, max_df=1.0, max_features=None,\n",
       "                                 min_df=1, ngram_range=(2, 4), norm='l2',\n",
       "                                 preprocessor=None, smooth_idf=True,\n",
       "                                 stop_words=None, strip_accents=None,\n",
       "                                 sublinear_tf=False,\n",
       "                                 token_p...\n",
       "                 SGDClassifier(alpha=0.0001, average=False,\n",
       "                               class_weight='balanced', early_stopping=False,\n",
       "                               epsilon=0.1, eta0=0.0, fit_intercept=True,\n",
       "                               l1_ratio=0.15, learning_rate='optimal',\n",
       "                               loss='modified_huber', max_iter=1000,\n",
       "                               n_iter_no_change=5, n_jobs=None, penalty='l2',\n",
       "                               power_t=0.5, random_state=42, shuffle=True,\n",
       "                               tol=0.001, validation_fraction=0.1, verbose=0,\n",
       "                               warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "pipe.fit(train_df.sentence, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Оценим точность классификации на обучающей выборке"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 15min 50s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "predictions = pipe.predict(train_df.sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9267296048572796"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "balanced_accuracy_score(y, predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Построим confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "plot_confusion_matrix only supports classifiers",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[1;32mC:\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\_plot\\confusion_matrix.py\u001b[0m in \u001b[0;36mplot_confusion_matrix\u001b[1;34m(estimator, X, y_true, labels, sample_weight, normalize, display_labels, include_values, xticks_rotation, values_format, cmap, ax)\u001b[0m\n\u001b[0;32m    181\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    182\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_classifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 183\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"plot_confusion_matrix only supports classifiers\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    184\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    185\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: plot_confusion_matrix only supports classifiers"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1440x1440 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%time\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import plot_confusion_matrix\n",
    "class MyEstimator():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        return predictions\n",
    "    \n",
    "estimator = MyEstimator()\n",
    "    \n",
    "plt.figure(figsize=(20, 20))\n",
    "\n",
    "plot_confusion_matrix(estimator, train_df.sentence.values, y, display_labels=label_encoder.classes_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выведем по пять самых важных токенов для каждого языка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'SGDClassifier' object has no attribute 'intercept'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-39402bbec902>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpipe\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"model\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mintercept\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'SGDClassifier' object has no attribute 'intercept'"
     ]
    }
   ],
   "source": [
    "pipe[\"model\"].intercept"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Прогоним тестовые данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"data/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 23min 50s\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>sentence</th>\n",
       "      <th>language</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>תודה לכם .</td>\n",
       "      <td>he</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Precisamos de compaixão para começar , e auto-...</td>\n",
       "      <td>pt-br</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>這個增長相當大 ， 並且它將引發經濟的增長 。</td>\n",
       "      <td>zh-tw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>시애틀에서 자란 제가 처음 가난을 보게 되던 때를 기억해요 .</td>\n",
       "      <td>ko</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>これをロボットに組み込みました</td>\n",
       "      <td>ja</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>所以他們拿了紅綠藍 ， 不是只拿訂單上的部分貨物 。</td>\n",
       "      <td>zh-tw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>Van , aki felelősségteljesen .</td>\n",
       "      <td>hu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>Geen enkele , want Disney is niet van plan om ...</td>\n",
       "      <td>nl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>Я хотел вас оставить сегодня с идеей , что есл...</td>\n",
       "      <td>ru</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>Najuzbudljivija je Harlem dječja zona , koja v...</td>\n",
       "      <td>hr</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                           sentence language\n",
       "0      0                                         תודה לכם .       he\n",
       "1      1  Precisamos de compaixão para começar , e auto-...    pt-br\n",
       "2      2                            這個增長相當大 ， 並且它將引發經濟的增長 。    zh-tw\n",
       "3      3                 시애틀에서 자란 제가 처음 가난을 보게 되던 때를 기억해요 .       ko\n",
       "4      4                                    これをロボットに組み込みました       ja\n",
       "5      5                         所以他們拿了紅綠藍 ， 不是只拿訂單上的部分貨物 。    zh-tw\n",
       "6      6                     Van , aki felelősségteljesen .       hu\n",
       "7      7  Geen enkele , want Disney is niet van plan om ...       nl\n",
       "8      8  Я хотел вас оставить сегодня с идеей , что есл...       ru\n",
       "9      9  Najuzbudljivija je Harlem dječja zona , koja v...       hr"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "prediction = pipe.predict(train_df.sentence)\n",
    "train_df['language'] = label_encoder.classes_[prediction]\n",
    "train_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[['index',  'language']].to_csv('data/SmirnovGS.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Замечание\n",
    "Ниже приведен пример того, как можно добавить признаки на основе word2vec (в моих запусках это не дало прироста точности)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для начала разбиваем данные на слова"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import RegexpTokenizer\n",
    "from collections import defaultdict\n",
    "dic = defaultdict(set)\n",
    "def tokenize(dataset, tokenized):\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    for i in range(len(dataset)):\n",
    "        tokenized.append(tokenizer.tokenize(dataset.sentence[i].lower()))\n",
    "tokenized = []\n",
    "tokenize(train_df, tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучаем word2vec-модель на полученных токенах"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "size = 100\n",
    "w2v_model = Word2Vec(tokenized, min_count = 1, workers = 8, size=size) \n",
    "w2v_model.train(tokenized, total_examples = w2v_model.corpus_count, epochs = 3) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Воспользуемся полученной моделью. Каждое слово из текста с помощью моедели переведём в вектор и возьмем среднее арифметическое по всем таким векторам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert(data):\n",
    "    X = np.zeros((len(data), size))\n",
    "    for i in np.arange(len(data)):\n",
    "        print(i)\n",
    "        line = [tok for tok in data[i] if tok in w2v_model.wv]\n",
    "        length = len(line)\n",
    "        for word in line:\n",
    "            X[i] += w2v_model.wv[word]/length\n",
    "    return X\n",
    "matrix = convert(tokenized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Добавить полученную матрицу к исходным признакам можно например следующим образом"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class w2vTransformerTrain():\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return matrix\n",
    "    \n",
    "union_pipe = FeatureUnion(transformer_list=[\n",
    "          ('vectorizer', TfidfVectorizer(analyzer=\"char_wb\", ngram_range=(2, 4))),\n",
    "          ('w2v', w2vTransformerTrain())\n",
    "          ])\n",
    "\n",
    "pipe = Pipeline([\n",
    "    ('feat_union', union_pipe),\n",
    "    ('model', SGDClassifier(random_state=42, loss='modified_huber', class_weight='balanced', n_jobs=-1, verbose=1))\n",
    "    ], verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Помимо непосредственно векторов, полученных из word2vec модели, можно рассчитать центры для каждого языка и посчитать косинунсное расстояние вектора текста до каждого центра"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для начала расчитаем центры"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lang_count = (\n",
    "    train_df.language.value_counts()\n",
    "    .to_frame()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "centres = defaultdict(np.array)\n",
    "\n",
    "for lang in lang_count.keys():\n",
    "    centres[lang] = np.zeros(size)\n",
    "    \n",
    "for i in range(len(matrix)):\n",
    "    print(i)\n",
    "    centres[target[i]] += matrix[i] / lang_count[target[i]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее, расчитаем расстояния до каждого центра"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_dist = np.zeros((len(matrix), len(lang_count)))\n",
    "for i, vec in enumerate(matrix):\n",
    "    for j, lang in centres.keys():\n",
    "        cosine_dist[i][j] = cosine(centres[lang], vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Полученную матрицу можно так же добавить к обучающей выборке, получится некоторое подобие стеккинга, так как косинусное расстояния до центров каждого языка уже с некоторой точностью предсказывают к какому языку нужно отнести текст"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
